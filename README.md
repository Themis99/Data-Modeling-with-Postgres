# Data Modeling with Postgres 
## Introduction


### Schema 
We have chosen to create a Star schema for our database. (giati)
Our schema consists of one Fact Table and 5 Dimention Tables:

Fact table: 
- **songplays**: records in log data associated with song plays. Includes the columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimention Tables 

- **users**: Users in the app. Includes the columns: user_id, first_name, last_name, gender, level
- **songs**: Songs in music database. Includes the columns: songs in music database
- **artists**: Artists in music database. Includes the columns: artist_id, name, location, latitude, longitude
- **time**: Timestamps of records in songplays broken down into specific units. Includes the columns: Start_time, hour, day, week, month, year, weekday

### ETL process

Step 1: The ETL process is done in the etl.py script. The loading of the data for the song and log dataset is done with the help of the Pandas library. 

Step 2: Then the necessary transformations are done on the data (if needed) and the corresponding columns are selected for each table. 

Step 3: Finally the corresponding INSERT query is executed to load the data into the tables of the database. 

Step 4: The above procedure is repeated for each JSON file to fill the tables

### Data

We use two datasets the Song Dataset and the logs dataset:

- **The Song Dataset**: In this dataset each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
- **The log Dataset**: This dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month.

### scripts

- **sql_queries**: This script contains all CREATE, INSERT, and DROP queries for all tables
- **create_tables**: This script creates the database and uses the queries from sql_queries.py to create all the tables
- **etl.py**: This script extracts the data from the JSON files, does the necessary transformations and loads the data into the database tables

### How to run the scripts

In order to create the tables and complete the etl process please run these commands below with this order in a terminal:

python create_tables.py
python etl.y

### Example queries
